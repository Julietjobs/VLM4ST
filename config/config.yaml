# VLM4ST Configuration File

# Data Configuration
data:
  dataset_path: "./Dataset/dataset1.json"
  dataset_paths:
    - "./Dataset/dataset1.json"
    - "./Dataset/dataset2.json"
    # ... more datasets
  task: "prediction"
  normalize: true
  train_ratio: 0.7
  val_ratio: 0.15
  test_ratio: 0.15

# Model Architecture Configuration
model:
  # Input dimensions
  T: 6  # Temporal length for input (T_in)
  T_out: 6  # Temporal length for output/prediction (T_out)
  H: 32  # Spatial height
  W: 32  # Spatial width
  d_model: 256  # Feature dimension
  
  # DiX-Attention
  dix_attention:
    num_heads: 8
    num_layers: 2
    dropout: 0.1
  
  # PromptForge
  prompt_forge:
    num_coarse_prototypes: 5
    num_fine_per_coarse: 2
    d_prompt: 128  # Prompt dimension per modality (temporal/spatial/cross)
  
  # Gate Fusion
  gate_fusion:
    num_branches: 4  # temporal, spatial, cross, input
  
  # Format Transformer
  format_transformer:
    spatial_pool_size: 10
    num_latents: 300
    vlm_hidden_dim: 768
  
  # VLM (ALBEF)
  vlm:
    image_res: 224
    embed_dim: 256
    vision_width: 768
    bert_config: "./config/config_bert.json"
    # Download ALBEF checkpoint from: https://storage.googleapis.com/sfr-pcl-data-research/ALBEF/ALBEF.pth
    pretrained_path: "./checkpoints/pretrained/ALBEF_14M.pth"
    use_lora: true  # Enable LoRA fine-tuning
    lora_type: "standard"  # Options: "standard", "mtl", "moe", "task_adaptive"
    lora_rank: 16   # LoRA rank (r)
    lora_alpha: 32  # LoRA scaling (alpha)
    # Multi-task LoRA specific parameters
    num_tasks: 14  # Number of datasets/tasks for multi-dataset training
    num_experts: 4  # Number of experts for MoE/Task-Adaptive LoRA
    task_emb_dim: 32  # Task embedding dimension for gate network
  
  # Prediction Head
  prediction_head:
    hidden_dim: 512
    output_channels: 1  # Single channel output

# Training Configuration
training:
  batch_size: 32
  num_epochs: 65
  learning_rate: 1.0e-4
  weight_decay: 0.05
  warmup_epochs: 5
  
  # Loss weights
  loss:
    mse_weight: 1.0          # Prediction loss (MSE)
    mae_weight: 0.5          # Prediction loss (MAE)
  
  # Optimizer
  optimizer: "adamw"
  scheduler: "cosine"
  
  # Device
  device: "cuda"
  num_workers: 4
  
  # Checkpoint
  save_dir: "./checkpoints"
  log_dir: "./logs"
  save_frequency: 5

# Testing Configuration
testing:
  checkpoint_path: "./checkpoints/best_model.pth"
  batch_size: 32
  output_dir: "./results"

# Few-shot/Zero-shot Configuration
few_shot:
  target_ratio: 0.0  # 0.0 for zero-shot, 0.05 for 5%, 0.1 for 10%, etc.
  num_epochs: 65     # Fewer epochs for few-shot training
  save_dir: "./checkpoints/few_shot"
  output_dir: "./results/few_shot"

